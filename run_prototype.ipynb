{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tools import EarlyStopping, adjust_learning_rate\n",
    "import xLSTMTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.seq_len = 9  # time length\n",
    "        self.input_size = 10  # num_features\n",
    "        self.target_len = 3\n",
    "        self.head_size = 16\n",
    "        self.num_heads = 2\n",
    "        self.proj_factor_slstm = 4/3\n",
    "        self.proj_factor_mlstm = 2\n",
    "        self.layer_type = \"msm\"\n",
    "        self.batch_first = True\n",
    "        self.linear_embed_dim = 64\n",
    "        self.split_ratio = 0.9\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 500\n",
    "        self.lr = 0.001\n",
    "        self.pct_start = 0.3\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.lradj = 'TST'\n",
    "        self.path = './results'\n",
    "        self.patience = 100\n",
    "\n",
    "configs = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/train_target_assemble(18-21).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시점 분리\n",
    "df_data = df[df.columns[1:]]\n",
    "df_stamp = df[df.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=int(len(df_data) * configs.split_ratio)\n",
    "train_set,test_set=df_data.iloc[:train_size],df_data.iloc[train_size:]\n",
    "print(f\"train size={len(train_set)}\\ntest_size={len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_columns=train_set.columns\n",
    "scaler=StandardScaler()\n",
    "scaler=scaler.fit(train_set[scaled_columns].values)\n",
    "train_set[scaled_columns] = scaler.transform(train_set[scaled_columns].values)\n",
    "test_set[scaled_columns] = scaler.transform(test_set[scaled_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, seq_len=8, target_len=8):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - seq_len - target_len):\n",
    "        x_data = dataset.iloc[i:(i + seq_len), :].values\n",
    "        dataX.append(x_data)\n",
    "        y_data = dataset.iloc[(i + seq_len):(i + seq_len)+target_len, :].values\n",
    "        dataY.append(y_data)\n",
    "    return torch.Tensor(dataX), torch.Tensor(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset(train_set, configs.seq_len, configs.target_len)\n",
    "X_test, y_test = create_dataset(test_set, configs.seq_len, configs.target_len)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=configs.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=configs.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xLSTMTime.Model(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, configs):\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    \n",
    "    valid_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(configs.device)\n",
    "            targets = targets.to(configs.device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            outputs = outputs.detach().cpu()\n",
    "            targets = targets.detach().cpu()\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            valid_losses.append(loss)\n",
    "    \n",
    "    valid_losses = np.average(valid_losses)\n",
    "    model.train()\n",
    "    return valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, configs):\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=configs.lr)\n",
    "    scheduler = lr_scheduler.OneCycleLR(optimizer = optimizer,\n",
    "                                            steps_per_epoch = len(train_loader),\n",
    "                                            pct_start = configs.pct_start,\n",
    "                                            epochs = configs.epochs,\n",
    "                                            max_lr = configs.lr)\n",
    "    early_stopping = EarlyStopping(patience=configs.patience, verbose=True)\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(configs.epochs), desc=f'Training', colour='GREEN'):\n",
    "        iter_count = 0\n",
    "        train_losses = []\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(configs.device)\n",
    "            targets = targets.to(configs.device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            adjust_learning_rate(optimizer, scheduler, epoch + 1, configs, printout=False)\n",
    "            scheduler.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"\\tepoch: {epoch+1} | iters: {i+1} | loss: {loss.item():.4f}\")\n",
    "                iter_count = 0\n",
    "\n",
    "        train_losses = np.average(train_losses)\n",
    "        valid_losses = validation(model, test_loader, configs)\n",
    "        print(f\"Epoch: {epoch+1}, Steps: {len(train_loader)} | Train Loss: {train_losses:.4f} Vali Loss: {valid_losses:.4f}\") \n",
    "        early_stopping(valid_losses, model, configs.path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        adjust_learning_rate(optimizer, scheduler, epoch + 1, configs)\n",
    "\n",
    "    best_model_path = configs.path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = train_model(model, train_loader, test_loader, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "test_files = glob.glob('./dataset/test_*_assemble.csv')\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for i, files in tqdm(enumerate(test_files)):\n",
    "    test_df = pd.read_csv(files)\n",
    "    test_df = test_df[test_df.columns[1:]]\n",
    "    \n",
    "    scaled_test = scaler.transform(test_df.values)\n",
    "\n",
    "    X_predict = torch.Tensor(scaled_test)\n",
    "    pred_dataset = TensorDataset(X_predict)\n",
    "    pred_loader = DataLoader(pred_dataset, batch_size=configs.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    trained_models.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in pred_loader:\n",
    "            inputs = inputs[0]\n",
    "            inputs = inputs.unsqueeze(dim=0)\n",
    "            inputs = inputs.to(configs.device)\n",
    "\n",
    "            outputs = trained_models(inputs)\n",
    "\n",
    "            outputs = outputs.detach().cpu()\n",
    "\n",
    "    df = pd.DataFrame(outputs.squeeze(0).numpy())\n",
    "    unscaled_df = pd.DataFrame(scaler.inverse_transform(df), columns=df_data.columns)\n",
    "    result_df = pd.concat([result_df, unscaled_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.rename(columns={'깐마늘':'깐마늘(국산)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_date = submission[submission.columns[0]]\n",
    "sub_data = submission[submission.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df[sub_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_result = pd.concat([sub_date, result_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_result.to_csv(f'sample_submission.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
